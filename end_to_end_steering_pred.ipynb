{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already preprocessed\n",
      "Total Processing tim : 9.393692016601562e-05s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"DATA PREPROCESSING\"\"\"\n",
    "\n",
    "from PIL import Image, ImageFilter, ImageEnhance, ImageFile\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def crop(img):\n",
    "    width, height = img.size\n",
    "    img = img.crop((0, 120, width, height))\n",
    "    return img\n",
    "\n",
    "\n",
    "def blur(img):\n",
    "    img = img.filter(ImageFilter.BLUR)\n",
    "    return img\n",
    "\n",
    "def darken(img):\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    img = enhancer.enhance(0.5) # value proportional to brightness\n",
    "    return img\n",
    "\n",
    "def preprocess(data_path, save_dir):\n",
    "    start = time.time()\n",
    "    if not save_dir.exists():\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(\"Save Dir created\")\n",
    "    \n",
    "\n",
    "        for file in data_path.iterdir():\n",
    "            if str(file.suffix) == \".jpg\":\n",
    "                img = Image.open(data_path/file)\n",
    "                img = crop(img)\n",
    "                img = blur(img)\n",
    "                img = darken(img)\n",
    "\n",
    "                print(f\"Saving : {save_dir/file.name}\")\n",
    "                img.save(save_dir/file.name)\n",
    "\n",
    "            if str(file.stem)==\"data\":\n",
    "                shutil.copy(str(file),str(save_dir/file.name) )\n",
    "\n",
    "    else:\n",
    "        print(\"Already preprocessed\")\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Total Processing tim : {end-start}s\")\n",
    "            \n",
    "\n",
    "def vis_image(img):\n",
    "    # plt.imshow(np.transpose(img,  (1, 2, 0)))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    \n",
    "\n",
    "    root_dir = Path(\"/content/drive/MyDrive/research\")\n",
    "    data_dir = Path(\"/content/data\")\n",
    "    data_path = root_dir/\"driving_dataset\"\n",
    "    save_dir = root_dir/\"driving_dataset_preprocessed\"\n",
    "\n",
    "    !apt install unzip\n",
    "    if not data_dir.exists():\n",
    "        !mkdir /content/data\n",
    "        !unzip /content/drive/MyDrive/research/driving_dataset.zip -d /content/data\n",
    "except:\n",
    "    root_dir=Path(\"/home/avishkar/Desktop/research\")\n",
    "    data_path=root_dir/\"driving_dataset\"\n",
    "    save_dir=root_dir/\"driving_dataset_preprocessed\"\n",
    "\n",
    "preprocess(data_path=data_path, save_dir=save_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imageio in /home/avishkar/.local/lib/python3.10/site-packages (2.34.1)\n",
      "Requirement already satisfied: numpy in /home/avishkar/.local/lib/python3.10/site-packages (from imageio) (1.26.3)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/avishkar/.local/lib/python3.10/site-packages (from imageio) (10.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32207/3186932329.py:36: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = iio.imread(self.data_dir/img_path)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABC80lEQVR4nO3deXiU5bk/8O87k5nJnhACWSQgi7IKltW4AAqy2GOhQAvaFqgUKgWLYEXxp7KpQVBxAyzHI1YLyMEKVFuxgoC1BAooIlo4grGgEEA0C1kms9y/PziZw5gE3jtkeBL4fq5rrovMPDzzvMvMPe/M835fS0QEREREF5jD9ACIiOjSxAJERERGsAAREZERLEBERGQECxARERnBAkREREawABERkREsQEREZAQLEBERGcECdAmbP38+2rVrh2AwaHooDc6sWbNgWZbpYVy0+vbti06dOtX6//v9fkyfPh1ZWVlwOBwYOnQoAMCyLMyaNatuBlkPjBo1Cj/96U9ND6PWWIDOYevWrZg1axYKCgpMD6VOFRUV4fHHH8d9990Hh+P/doOpU6eia9euSElJQWxsLNq3b49Zs2bh1KlTtvq1LKva27x588LarVmzBgMHDkRmZiY8Hg+aNWuGESNGYO/evdX2++c//xldu3ZFdHQ0mjdvjpkzZ8Lv91dpV1BQgAkTJqBJkyaIi4vDjTfeiA8//FCxZmrvsccew9q1ay/Ic0VaQ1+Wl156CQsWLMCIESPwhz/8AVOnTr0gz3vkyBHMmjULu3fvrrHNqlWrkJ2djbi4OCQnJ+Paa6/Fe++9V2P7Dz74IPQ6+uabb8Ieu++++/CnP/0JH3/8cV0twgUVZXoA9d3WrVsxe/ZsjB07FsnJyaaHU2deeukl+P1+3HbbbWH379ixAzfccAN++ctfIjo6Gh999BHmzZuHDRs24P333w8rVjW5+eabMXr06LD7fvCDH4T9/cknn6BRo0aYMmUKUlNTkZ+fj5deegk9e/ZEbm4uunTpEmr79ttvY+jQoejbty+ee+45fPLJJ3jkkUdw/PhxLFmyJNQuGAzihz/8IT7++GPce++9SE1NxeLFi9G3b1/s2rULV1xxRW1WlW2PPfYYRowYEfq03ZA19GV57733cNlll2HhwoUX9HmPHDmC2bNn4/LLL8fVV19d5fFZs2Zhzpw5GDFiBMaOHQufz4e9e/fi66+/rra/YDCIu+66C3FxcSgpKany+A9+8AN0794dTz75JF555ZW6XpzIEzqrBQsWCADJy8szPZTzdurUqdC/O3fuLD//+c9t/b8nnnhCAEhubu452wKQSZMm1Wp8+fn5EhUVJb/+9a/D7u/QoYN06dJFfD5f6L7/9//+n1iWJf/6179C961atUoAyOrVq0P3HT9+XJKTk+W2226r1ZhqMnPmTPn+yycuLk7GjBlTp89jSiSW5cz971z69OkjHTt2rPVz3XjjjdX+fwAyc+bMWvd7Ljt27BAAsmzZsiqP5ebmimVZ8tRTT9nub8mSJdK4cWOZMmWKAJATJ05UafPEE09IXFycFBcXn8/QjWABOovKN5nv384sRq+++qp07dpVoqOjpVGjRjJy5Eg5dOhQWD+VL6ZPP/1U+vbtKzExMZKZmSmPP/54led89tlnpUOHDhITEyPJycnSrVs3Wb58eVibDz/8UAYNGiQJCQkSFxcnN910U5XisGzZMgEgmzdvlokTJ0qTJk0kOTlZRES++OILASAvv/yyrfXw+uuvCwB5++23z9m2sgCVlpZKWVmZrf4rBYNBSUxMlJEjR4bu+/TTTwWALFq0KKzt119/LQBk7ty5oft+8pOfSFpamgQCgbC2EyZMkNjYWCkvL1eNp9Lf//536d69u3g8HmnVqpW88MILVQpQdfvJmDFj5L333hMA8sYbb1Tpd/ny5QJAtm7dKiIiY8aMkbi4ODl48KAMGDBAYmNjJSMjQ2bPni3BYDDs/wYCAVm4cKF06NBBPB6PNG3aVCZMmCDffvttrZbxTDUtS6Xz3f9ERP76179K7969JT4+XhISEqR79+5h+7nmNXOmvLy8ase/adOm0LJ9vwDZWZ6TJ0/KPffcI506dZK4uDhJSEiQQYMGye7du0NtNm3aVO1zVxajkSNHSkZGhgQCAQkGg+csGCdPnpTGjRvLokWLQvtbdQXo448/rnEfq+9YgM7i448/lttuu00AyMKFC+XVV1+VV199NfRJ7pFHHhHLsmTkyJGyePFimT17tqSmpsrll18u3333XaifPn36SGZmpmRlZcmUKVNk8eLFctNNNwkA+etf/xpqt3TpUgEgI0aMkN///vfyzDPPyLhx4+S3v/1tqM3evXslLi5OMjIyZO7cuTJv3jxp2bKleDwe2bZtW6hd5RtAhw4dpE+fPvLcc8/JvHnzRETkj3/8owCQPXv2VLvcPp9PTpw4IV9//bW888470q5dO0lISJCTJ0+ec50BkLi4OLEsSwBI+/btqxTQM3333Xdy/Phx2bNnj9xxxx0CQJYuXRp6vHKs27dvr/J/mzVrJsOGDQv93aZNGxk8eHCVdi+++OJZl/ds9uzZIzExMdK8eXPJycmRuXPnSlpamnTu3DmsAL366qvi8XjkhhtuCO0nW7dulWAwKFlZWTJ8+PAqfd9yyy3SunXr0N9jxoyR6OhoueKKK+QXv/iFPP/88/If//EfAkAeeuihsP/7q1/9SqKiomT8+PHywgsvyH333SdxcXHSo0cPqaioCLUrLi6WEydOnPNWUFBwzmURqZv9b9myZWJZlnTq1EkeffRRWbRokfzqV7+SX/ziF6H/b/c1832nTp2SV199Vdq1ayfNmjULjT8/P19EqhYgu8uzY8cOad26tdx///3y+9//XubMmSOXXXaZJCUlyddffy0ip4/g58yZIwBkwoQJoec+ePCgiIikpqbKj370I1m4cKE0btxYAEh6ero899xz1S7Lb37zG+nYsaP4/f6zFiCfzycxMTFyzz331Lhe6isWoHOo6Su4L7/8UpxOpzz66KNh93/yyScSFRUVdn+fPn0EgLzyyiuh+7xer6Snp4e9MQ0ZMuScXzsMHTpU3G53aKcWETly5IgkJCRI7969Q/dVvgFcf/314vf7w/p48MEHBUCNn8Byc3PDPsG1bds29AnyXK699lp5+umnZd26dbJkyRLp1KmTAJDFixdX275t27ah54mPj5cHH3ww7Aimcv1//6hSRKRHjx5yzTXXhP6Oi4uTO+64o0q7v/zlLwJA1q9fb2sZzjR06FCJjo6Wf//736H7PvvsM3E6nba/gpsxY4Z4PJ6wN/njx49LVFRU2JvhmDFjBIDcddddofuCwaD88Ic/FLfbHXrz+fvf/y4AqhT29evXV7m/ss9z3fr06WNrWc53/ysoKJCEhATp1atXlSPkM4/y7L5malLTV3jfL0B2l6e8vLzKkXVeXp54PB6ZM2dO6L6avoL79ttvBYA0btxY4uPjZcGCBbJq1SoZNGiQAJAXXnghrP3HH38sTqdT3nnnHRGRsxYgEZErr7yy2g9f9R0nIdTSG2+8gWAwiJ/+9KdhM1PS09NxxRVXYNOmTXjggQdC98fHx+PnP/956G+3242ePXviiy++CN2XnJyMr776Cjt27ECPHj2qPGcgEMDf/vY3DB06FK1atQrdn5GRgdtvvx3/+Z//iaKiIiQmJoYeGz9+PJxOZ1g/J0+eRFRUFOLj46tdtg4dOuDdd99FSUkJtm7dig0bNtieBfePf/wj7O877rgD3bp1wwMPPICxY8ciJiYm7PFly5ahqKgIX3zxBZYtW4aysjIEAoHQZIeysjIAgMfjqfJc0dHRKCoqCv1dVlZWY7sz+7IrEAjgnXfewdChQ9G8efPQ/e3bt8fAgQPx17/+1VY/o0ePRk5ODl5//XWMGzcOwOmZUH6/P2yfqDR58uTQvy3LwuTJk/GXv/wFGzZswKhRo7B69WokJSXh5ptvDtv3unXrhvj4eGzatAm33347AGD69OnVPsf3NWrU6Jxt6mL/e/fdd1FcXIz7778/tF3OXNYz2XnNnA/N8py5XwUCARQUFCA+Ph5t27a1Ncuy8vVz8uRJvPbaaxg5ciQAYMSIEbjqqqvwyCOP4Ne//nWo/W9/+1sMHjwYAwYMsLUsjRo1qjJDriFgAaqlzz//HCJS48wql8sV9nezZs2qvMAaNWqEPXv2hP6+7777sGHDBvTs2RNt2rTBgAEDcPvtt+O6664DAJw4cQKlpaVo27Ztledr3749gsEgDh8+jI4dO4bub9mypXrZEhMT0b9/fwDAkCFDsGLFCgwZMgQffvhh2Ow0O9xuNyZPnow777wTu3btwvXXXx/2eHZ2dujfo0aNQvv27QEATzzxBACECpbX663Sd3l5eVhBi4mJqbHdmX3ZdeLECZSVlVW7jdu2bWu7ALVr1w49evTA8uXLQwVo+fLluOaaa9CmTZuwtg6HI+zNEACuvPJKAMCXX34J4PS+V1hYiKZNm1b7fMePHw/9u0OHDujQoYOtcZ5LXex/Bw8eBABb5/jYec2cD83yBINBPPPMM1i8eDHy8vIQCARCbRs3bnzO56rc91wuF0aMGBG63+FwYOTIkZg5cyYOHTqE5s2bY9WqVdi6dWuNpyRUR0Qa5HlpLEC1FAwGYVkW3n777SpHGACqHF1U1wY4veNUat++Pfbv34+33noL69evx5/+9CcsXrwYDz/8MGbPnl2rcVb3ptu4cWP4/X4UFxcjISHhnH0MGzYMv/jFL/Daa6+pCxAAZGVlAQC+/fbbs7Zr1KgRbrrpJixfvjxUgDIyMgAAR48eDfVT6ejRo+jZs2fo74yMDBw9erRKv5X3ZWZmqsdeV0aPHo0pU6bgq6++gtfrxbZt2/D888/Xqq9gMIimTZti+fLl1T7epEmT0L8LCwttHfm53W6kpKTUajxnoy36Z7LzmrlQHnvsMTz00EO44447MHfuXKSkpMDhcODuu++2dSJ3SkoKoqOjkZycXGW5Kj9IfPfdd2jevDnuvfde/OQnP4Hb7Q596Kg8D/Hw4cOoqKiosi9/9913ET/NIBJYgM6hpk8VrVu3hoigZcuWoU+odSEuLg4jR47EyJEjUVFRgWHDhuHRRx/FjBkz0KRJE8TGxmL//v1V/t++ffvgcDiqvElXp127dgCAvLw8dO7c+ZztvV4vgsEgCgsL9QsEhL4yOfONsSZlZWVhz1N5LsXOnTvDis2RI0fw1VdfYcKECWFt//73vyMYDIadr7R9+3bExsaqt1OTJk0QExODzz//vMpj1W2Ds30CHTVqFKZNm4aVK1eirKwMLpcr9DXMmYLBIL744ouwsf7P//wPAODyyy8HcHrf27BhA6677rpzvsFPmTIFf/jDH87aBgD69OmDzZs3n3VZ6mL/a926NQBg7969VY7+LjTN8rz++uu48cYb8V//9V9h7QoKCpCamhr6u6Z9wOFw4Oqrr8aOHTtQUVEBt9sdeuzIkSOh8QCni8yKFSuwYsWKKv107doVXbp0CTvR1e/34/Dhw/jRj35kc8nrDyYhnENcXBwAVElCGDZsGJxOJ2bPnl3lE5mI4OTJk+rn+v7/cbvd6NChA0QEPp8PTqcTAwYMwLp160KfjADg2LFjWLFiBa6//vqw799rUvm1186dO8PuLygogM/nq9L+xRdfBAB07949dF9paSn27dsX9r3ziRMnqvzf4uJiPP3000hNTUW3bt1C95/5NVGlL7/8Ehs3bgx7no4dO6Jdu3ZYunRp2NceS5YsgWVZYV9njBgxAseOHcMbb7wRuu+bb77B6tWrceutt1b7+9DZOJ1ODBw4EGvXrsWhQ4dC9//rX//CO++8U6V9XFxcjYkZqampGDx4MP74xz9i+fLlGDRoUNgb15nOPDISETz//PNwuVzo168fAOCnP/0pAoEA5s6dW+X/+v3+sDFMnz4d77777jlvTz755DmXpS72vwEDBiAhIQE5OTmhr0bPXNba2LdvX9j2sUuzPE6ns8r4Vq9eXeUE0preLwBg5MiRCAQCYR8IysvLsXz5cnTo0CF0VLNmzZoqt8oPK6+88kqVk2s/++wzlJeX49prr1WvA+MMTX5oMP75z38KALnlllvklVdekZUrV4amYefk5AgAufbaa2X+/PmyZMkSmT59ulxxxRWyYMGCUB81zcgZM2aMtGjRIvR3165d5ZZbbpFHH31UXnzxRbnnnnvE4/HIrbfeGmpTOW30sssuk0cffVQef/xxadWqVY3TYHfs2FHtcnXq1KnKyZlr1qyRrKwsmTp1qixevFiefvppGT58uFiWJd27dxev1xtqW3nOw5kzimbOnCldunSRBx98UJYuXSqzZ8+WFi1aiGVZ8sc//jHsuZo2bSq33XabPP7447J06VK59957JSUlRaKjo+Uf//hHWNs333xTLMuSm266SZYuXSq//e1vxeFwyPjx48Pa+f1+ueaaayQ+Pl5mz54tixYtko4dO0pCQoLs27evyrqHjROMP/74Y4mOjpbmzZvLvHnz5JFHHql2GrbI6WnVcXFx8uSTT8rKlSvDtofI/51PBUBWrVpV5bnOnIY9evRoWbRoUWga9gMPPBDW9te//rUAkMGDB8vChQvl+eeflylTpkhmZmbYibi1VdOy1MX+VzktvlOnTvLYY4/JkiVL5M4775TRo0eH2th9zYhItbP47M6Cs7s8Dz/8sACQsWPHytKlS+Wuu+6SlJQUadWqVdhzV1RUSHJysrRt21ZefPFFWblypXzxxRciIlJaWiodO3YUl8slv/vd7+TZZ5+VHj16iNPpPOvUcpGzz4J74oknJDY2VoqKis7aR33EAmTD3Llz5bLLLhOHw1HlTetPf/qTXH/99RIXFydxcXHSrl07mTRpkuzfvz/Uxu6L6fe//7307t1bGjduLB6PR1q3bi333nuvFBYWhv2/Dz/8UAYOHCjx8fESGxsrN954Y+g8jUrnKkBPPfWUxMfHS2lpaei+AwcOyOjRo6VVq1YSExMj0dHR0rFjR5k5c2aVs9irK0B/+9vf5Oabb5b09HRxuVySnJwsAwYMkI0bN1Z5/pkzZ0r37t2lUaNGEhUVJZmZmTJq1Kgaz9VZs2aNXH311eLxeKRZs2by4IMPhp3vUunbb7+VcePGSePGjSU2Nlb69OlT7ToYPny4xMTEhJ2vVZMtW7ZIt27dxO1213giqojIvn37pHfv3hITE1Pl5E2R09OIGzVqJElJSdWepFvdiahpaWkyc+bMKlOARU6fN9atWzeJiYmRhIQEueqqq2T69Oly5MiRcy7TuZxtWepi//vzn/8s1157rcTExEhiYqL07NlTVq5cGXr8QhUgu8tTXl4u99xzj2RkZEhMTIxcd911kpubK3369Kny3OvWrZMOHTpIVFRUlSnZx44dkzFjxkhKSop4PB7p1auXrdMDzlaAevXqZTvVpL6xRAz8okfGFRYWolWrVpg/f35oZtalJC0tDaNHj8aCBQsu2HP6/X5kZmbi1ltvrfJbAgCMHTsWr7/+uu0p70S7d+9G165d8eGHH1abPVff8TegS1RSUhKmT5+OBQsWXHKXY/j0009RVlaG++6774I+79q1a3HixIkqQa1EtTVv3jyMGDGiQRYfAOAREFGEbd++HXv27MHcuXORmppa44mLPAKiSw2PgIgibMmSJZg4cSKaNm3aMCPziSKER0BERGQEj4CIiMgIFiAiIjKi3kXxBINBHDlyBAkJCQ0yXI+I6FInIiguLkZmZmZYLNb31bsCdOTIEVt5ZkREVL8dPnwYzZo1q/HxeleAKtOZGzVKtn0EpJlHEQjoznkRsd8+Ksp17kZncHvc524U6lu3qSq8Fbbb+v1V89/ORjtvRXMg61dun9PJNvYkxsfqerbsj6W0VLcOcZZPhdVpmtnCdtvmLXUhn/lf2Y/9L/y2QNW302l/H7f8yvlQzsC521T2rVzfFT6/qn15mf32brfutRzlsj/28jL7r3sAiI8/dxp+pWhP9enk1QkEgti/74tzpu1HrAAtWrQICxYsQH5+Prp06YLnnnsuLM24JpVFx7IsOBz23rk051Hqv9az3/5sh5rn2z6yfevWifa8Vc0qj+TXrtp1KJpxK9ehtgDVdGmC6kS5dB+ENH2r90On/fZWUFuA7LdXjzvQUF/L2v3KfnvNflLpXK/niExCWLVqFaZNm4aZM2eGLmI2cODAahOQiYjo0hSRAvTUU09h/Pjx+OUvf4kOHTrghRdeQGxsLF566aUqbb1eL4qKisJuRER08avzAlRRUYFdu3aFLukMnD4s7N+/P3Jzc6u0z8nJQVJSUujGCQhERJeGOi9A33zzDQKBANLS0sLuT0tLQ35+fpX2M2bMQGFhYeh2+PDhuh4SERHVQ8ZnwXk8HvWVKomIqOGr8yOg1NRUOJ1OHDt2LOz+Y8eOIT09va6fjoiIGqg6L0ButxvdunXDxo0bQ/cFg0Fs3LgR2dnZdf10RETUQEXkK7hp06ZhzJgx6N69O3r27Imnn34aJSUl+OUvfxmJpyMiogYoIgVo5MiROHHiBB5++GHk5+fj6quvxvr166tMTDBBk2xwur0mZcH+mdkAEFSc9R906E7S8/vtn5mtTYfQnJwLKE9EVZ7QGfDbX+dl5eWqvi3FSXo+5Vn8DodunRcXfWO77eE83dnwfm+J/cbqi7fY/w/a12ZFuf32TsVJqwDgU7x+AN37hPoCOIozot1u+8kTABAdY78EiOJEYbG53SM2CWHy5MmYPHlypLonIqIGjpdjICIiI1iAiIjICBYgIiIyggWIiIiMYAEiIiIjWICIiMgIFiAiIjKCBYiIiIxgASIiIiOMX46hJiIBBIP2IiiCigSPoKYxdBEb2kuma67fromzAXSxJtoIFOUqhMNhf8Vor2kfUMSa+LWRQ5b9sbjcsaquAz6vrn2F/fZlxQWqvi2H/TijgHLjuxQ7bgCRi78J2HwvqeT3aV8T9seifQ+qqLDfd1SU7k0oGPTZbuursF8u7KaS8QiIiIiMYAEiIiIjWICIiMgIFiAiIjKCBYiIiIxgASIiIiNYgIiIyAgWICIiMoIFiIiIjGABIiIiI1iAiIjIiHqbBefz+WHZzpGyn/Okz4LTtFU0BmA57I/b4dRtKrfbY7ttaan9PChAl3sFACL2P+do16Fi0wPari37uVpxcQmqvsuKdYMJBuwvaEWFbh/3RNvvO9rjVvXtVAQkijJL0anImRP1Pqsbi6h2RN3nftXQlaGRAcV+ZTli7LcVe2FwPAIiIiIjWICIiMgIFiAiIjKCBYiIiIxgASIiIiNYgIiIyAgWICIiMoIFiIiIjGABIiIiI1iAiIjIiHobxRMMiu1UCU18izZGxn4ckD5GRtPc4dDllERF2d+0TqdL1XcwqIvu0awXdZyRpq0upQQuxTr3uHTbR6KjVe19gQpF56qu4XLaj22K8dhve5oiQsjSfR52+O1HDvn99qJhKkW5dPt4fEKy7bZxMfYjbQBdnFF0tG4/jI21Pxax7G97v9+HA/v3nrMdj4CIiMgIFiAiIjKCBYiIiIxgASIiIiNYgIiIyAgWICIiMoIFiIiIjGABIiIiI1iAiIjICBYgIiIyggWIiIiMqLdZcKczpOzlSGkzvjR0OXP2s6m07QN+v6pvh8P+Z4v4+DhV3yUlpar2gYD9sYsq3Q2wFDlZTpcux0yTv1dWVqbqWxUECCDKaX+9uFxuVd8J8cm22yYm6PYVTRact1yRdwcAlv3l9Ad0r83CokJV+xbN29huq80N9Cjy3aJcyjw9h/3t4wvYz9Pz+exl6fEIiIiIjKjzAjRr1ixYlhV2a9euXV0/DRERNXAR+QquY8eO2LBhw/89ieLSAEREdGmISGWIiopCenp6JLomIqKLRER+A/r888+RmZmJVq1a4Wc/+xkOHTpUY1uv14uioqKwGxERXfzqvAD16tULL7/8MtavX48lS5YgLy8PN9xwA4qLi6ttn5OTg6SkpNAtKyurrodERET1kCXaayArFRQUoEWLFnjqqacwbty4Ko97vV54vd7Q30VFRcjKykJsbKzqcth2BRRTCbW0v3XFxcfbbut266YQW4rrMovopqhGdhq2kmIX0a7DqCj703wdysuaQ3lpeFj216HHo5uGndq4ke22nIZdPU7D/l5bnw9vrH4DhYWFSExMrLFdxGcHJCcn48orr8SBAweqfdzj8cCjvs48ERE1dBE/D+jUqVM4ePAgMjIyIv1URETUgNR5Afrd736HLVu24Msvv8TWrVvx4x//GE6nE7fddltdPxURETVgdf4V3FdffYXbbrsNJ0+eRJMmTXD99ddj27ZtaNKkSV0/Va1of1eKZBRPhdf+d96WpfveOD4u2nbbBOX3+nFxuvZer/3fL/zK36M0OUwul/J3Gg3R7VcO5U9AsXH2f++IitItZ0y0/e0Z5dT9vqT5jc6p+K0DAIKw395S/kYXUL6WY6Ptv96cyo3vtBTtlePW0Px0JTZ/LqrzAvTaa6/VdZdERHQRYhYcEREZwQJERERGsAAREZERLEBERGQECxARERnBAkREREawABERkREsQEREZAQLEBERGcECRERERkT8cgy15XA4bOe2aa5o5HTqam5Q0bnDoaznmlw6Zd+emFjbbWPi7LcFgORGjVXtLcXnnFOlZaq+fX771yiJcul2d6dTkTWmzBj0aPK9ALgUY6+I3CWvAGUmYUys/Yw0d5RuHy+s4SKX1Skv1+1XEtStRJ+/xHbboKVbTksRqBel2GcBIKi4TpJPcV0vn89nqx2PgIiIyAgWICIiMoIFiIiIjGABIiIiI1iAiIjICBYgIiIyggWIiIiMYAEiIiIjWICIiMgIFiAiIjKi3kbxuFwu29E2IooYFGVkSjCoiExRxuU4o9z2u3babwsApV77ERuBAvsxIgAQ5ShXtQ8E7MeaeNU5Mva3Z3S0R9Wzy20/1iQuRrd9EmJdqvZl9lNQ1Pu4JssqqHmtAfArol7iE+JUfafHxdtuW1x8StW3NraprMx+/8Gg/XUCAG7FWGJjdPu4I1JvnWLvdcwjICIiMoIFiIiIjGABIiIiI1iAiIjICBYgIiIyggWIiIiMYAEiIiIjWICIiMgIFiAiIjKCBYiIiIxgASIiIiPqbRZcTFys7Sw4y7Kf2WU57bcFgCiXJuNL17fDYb+9Q/lZocLvs922pFQTNAZIsELXXhEiFVTkkgEAFFljFRW6ccco8t0axeq2T0C5r3j9mvWiy2sTRd6hJtcPAPw+++ulvFy3feJiY223TU5K1vUdbz9nDgDKSuznKZYo2gK6/ba4RJd5JzYz2wAg4LP/nuL323tP4REQEREZwQJERERGsAAREZERLEBERGQECxARERnBAkREREawABERkREsQEREZAQLEBERGcECRERERrAAERGREfU2C+6Ktm0RFWV3ePbraADKrDFF335l1pgocsx8fl0GV7nX/lgcLt1uEKXN01O0D1TYz5sCdNlkPkU+HgDER9sfd3KcR9V3cYUury2g2Fcs5T4uivw9bZ6ey+Wy3bbguyJV36WlpbbbxkbHqPqOVuTMAYDHY3/7u92afEnAH7S/7UtO6XLmvOVltttatt+PAQvMgiMionpMXYDef/993HrrrcjMzIRlWVi7dm3Y4yKChx9+GBkZGYiJiUH//v3x+eef19V4iYjoIqEuQCUlJejSpQsWLVpU7ePz58/Hs88+ixdeeAHbt29HXFwcBg4ciPLy8vMeLBERXTzUvwENHjwYgwcPrvYxEcHTTz+NBx98EEOGDAEAvPLKK0hLS8PatWsxatSo8xstERFdNOr0N6C8vDzk5+ejf//+ofuSkpLQq1cv5ObmVvt/vF4vioqKwm5ERHTxq9MClJ+fDwBIS0sLuz8tLS302Pfl5OQgKSkpdMvKyqrLIRERUT1lfBbcjBkzUFhYGLodPnzY9JCIiOgCqNMClJ6eDgA4duxY2P3Hjh0LPfZ9Ho8HiYmJYTciIrr41WkBatmyJdLT07Fx48bQfUVFRdi+fTuys7Pr8qmIiKiBU8+CO3XqFA4cOBD6Oy8vD7t370ZKSgqaN2+Ou+++G4888giuuOIKtGzZEg899BAyMzMxdOjQuhw3ERE1cOoCtHPnTtx4442hv6dNmwYAGDNmDF5++WVMnz4dJSUlmDBhAgoKCnD99ddj/fr1iI6OVj1PWpN0uGxGVpSW2Y+TKPfqzkfy+ezHtziUETU+RWJKlEsX3ZISZz9KxH7k0WnadRjlsD/2mARdBIpDMfaiQt0MyxSPvTgRAAgEddunQpeshKCmvW4oUCTxIKCIhTnd3n7n/oAuQsh7yv7rvqzMq+rbrYj5AQCHw/6XSU7l+0RycpLttknKnzDKFOdnlpbZb2v3fVNdgPr27XvW7CjLsjBnzhzMmTNH2zUREV1CjM+CIyKiSxMLEBERGcECRERERrAAERGRESxARERkBAsQEREZwQJERERGsAAREZERLEBERGQECxARERmhjuK5UAoLT8LlctlqGwjYD8oK+O3newGAXxHapRkHAES57K/+xBhdRpooMrvKS0t0fft0uVp+2B9Lmc9+vhegy9+LUX7cilFEdhWU67a9E7rcM3HY7/9sUVnVtldsH2XXEL/9YDq3NksxqMhpFF3ffq/ufUKzWixLF9YXCNjPYEtISFD1nRgfb7ttrMd+24qKClvteARERERGsAAREZERLEBERGQECxARERnBAkREREawABERkREsQEREZAQLEBERGcECRERERrAAERGREfU2iqestAy+KHtRG06nItpCmSUSCNiP5PD57EeDAIDHo1j9ouvbV2E/Lsdh2Y9iAYBYt72IpBDF5rGUH4kciv/g1qWxwBdU7CsOXbxKFHTRPZYiLicQ0G1PTXtligz8itgmS/l2pFvlunViaXZaaOOPdH1XeO3F2gDAt77vVH2XnTplu21stMd2W7vvhTwCIiIiI1iAiIjICBYgIiIyggWIiIiMYAEiIiIjWICIiMgIFiAiIjKCBYiIiIxgASIiIiNYgIiIyAgWICIiMqLeZsEVFpbB6bQ3vCi3/TrqcukW2VLUaIdDl5FWUmo/46lIkdkEAJpkKoc298qvzNNT5HA5lJlq7ij7AW8OZZCZJg8sqMwY9Ad12WQ+RV5bMKAbS1CReafNSHM47fftrNDl4zkV+4pDmQOoDr3TjEXZtyNov71y1PD57L8HFZeW227r99vL0OQREBERGcECRERERrAAERGRESxARERkBAsQEREZwQJERERGsAAREZERLEBERGQECxARERnBAkREREbU2ygeOCzb8RaKJBEERRsloqnRur41ESjicKv6thTRMAHNCoQuWgcAyr324z68ZfbjPgBARDEWZU6JU5Hfok16gUP32S+oiG/RbHstSznuKJtxWgDgVPbtjLLfPkr5ureUkVCW2I8RCjp1fTthv2+npT2msL+vaN6v7L6n8AiIiIiMYAEiIiIj1AXo/fffx6233orMzExYloW1a9eGPT527FhYlhV2GzRoUF2Nl4iILhLqAlRSUoIuXbpg0aJFNbYZNGgQjh49GrqtXLnyvAZJREQXH/UkhMGDB2Pw4MFnbePxeJCenl7rQRER0cUvIr8Bbd68GU2bNkXbtm0xceJEnDx5ssa2Xq8XRUVFYTciIrr41XkBGjRoEF555RVs3LgRjz/+OLZs2YLBgwcjEKh+KmFOTg6SkpJCt6ysrLoeEhER1UN1fh7QqFGjQv++6qqr0LlzZ7Ru3RqbN29Gv379qrSfMWMGpk2bFvq7qKiIRYiI6BIQ8WnYrVq1QmpqKg4cOFDt4x6PB4mJiWE3IiK6+EW8AH311Vc4efIkMjIyIv1URETUgKi/gjt16lTY0UxeXh52796NlJQUpKSkYPbs2Rg+fDjS09Nx8OBBTJ8+HW3atMHAgQPrdOBERNSwqQvQzp07ceONN4b+rvz9ZsyYMViyZAn27NmDP/zhDygoKEBmZiYGDBiAuXPnwuPxqJ7HaUXB6bA3PIcmW0mbwaXJmVPkKgGAWIosuMjFe6lzr6JsbpdKcc5Y222j3brMO6/Xfnacz+9T9e3z2m/v9dnP6wIAh3I/VDZX0Wx9VTQigKDb/n8Qh24nt8Rlu63DoXv/iXbp9kOnJqvPocs7dCny9FyKfLzTY1E0Vmwen89eRqO6APXt2xdylnfDd955R9slERFdgpgFR0RERrAAERGRESxARERkBAsQEREZwQJERERGsAAREZERLEBERGQECxARERnBAkREREawABERkRF1fj2guhLwewGxl6/lcCgyoYL2Mor+r29FY12kGmBpxqIbtyKaCg7twJXNNblnTkuXwRUMOhVtdXltwYBf0bcuZ04VrAUgqNhvg8rgwLNFa32f9hOry/7mgVORjQgATkWGoWXptr3TWaFqH1Ts45p8SQBwKjLyHJbyLV0Urx/FuO025REQEREZwQJERERGsAAREZERLEBERGQECxARERnBAkREREawABERkREsQEREZAQLEBERGcECRERERtTbKJ5oj4WoKHtRGx6P/UiOaJdukTVxH0Gb0UEhimgLSx2XY7+9OopHdLFAquQRZVSS5iNUUHTLGXTa31ckoMicARBQLqdXsW8F/PYjhABddI9fGSNTEbS/gTTxUf/7P2y3DFjKz9pe3WBcTsX2Vy6oVW6/rVO5mJYi/kgU47a7C/IIiIiIjGABIiIiI1iAiIjICBYgIiIyggWIiIiMYAEiIiIjWICIiMgIFiAiIjKCBYiIiIxgASIiIiNYgIiIyIh6mwWn4Q/Yz9XyQpk1pkgyCwR1WXCiyODStAV0+V6izIJTZ8cpMqQCAW2env3lDCpzzAIBzbbXda7YZdXt/QFttp/99qJL9lPtKZY2I02R06jJPNP2Deiy/aKidG+7lmU/Z06UeYcOh/1jEM17UMDmOHgERERERrAAERGRESxARERkBAsQEREZwQJERERGsAAREZERLEBERGQECxARERnBAkREREawABERkRH1NoqnuCwIp9NeLEuMx35UhdehjExRRPdo43I0xNJ9VtCMRRncAqfD/voGNGFG+rgcSxM5pI7LsR8LFFT2HVREt2j7j+BuqO5bE92j3Q8txTrRxvw4lJFD/qBf1V7D6dSuGfu00Uq2+7W5f/MIiIiIjGABIiIiI1QFKCcnBz169EBCQgKaNm2KoUOHYv/+/WFtysvLMWnSJDRu3Bjx8fEYPnw4jh07VqeDJiKihk9VgLZs2YJJkyZh27ZtePfdd+Hz+TBgwACUlJSE2kydOhVvvvkmVq9ejS1btuDIkSMYNmxYnQ+ciIgaNkvO45fzEydOoGnTptiyZQt69+6NwsJCNGnSBCtWrMCIESMAAPv27UP79u2Rm5uLa665pkofXq8XXq839HdRURGysrLQ4aqucDrt/dgd43HbHrOl/AFdNwlBe60h+0T5I2rDnYSgW4cRnYSguDaRflJB5Npr+9aI5EQb9fWAFM3VkxAU18k5zf560V4PyOl02W6rHbZ+Oe0J+P3Y9c/tKCwsRGJiYs3Pfz5PUlhYCABISUkBAOzatQs+nw/9+/cPtWnXrh2aN2+O3NzcavvIyclBUlJS6JaVlXU+QyIiogai1gUoGAzi7rvvxnXXXYdOnToBAPLz8+F2u5GcnBzWNi0tDfn5+dX2M2PGDBQWFoZuhw8fru2QiIioAan1eUCTJk3C3r178cEHH5zXADweDzwez3n1QUREDU+tjoAmT56Mt956C5s2bUKzZs1C96enp6OiogIFBQVh7Y8dO4b09PTzGigREV1cVAVIRDB58mSsWbMG7733Hlq2bBn2eLdu3eByubBx48bQffv378ehQ4eQnZ1dNyMmIqKLguoruEmTJmHFihVYt24dEhISQr/rJCUlISYmBklJSRg3bhymTZuGlJQUJCYm4q677kJ2dna1M+CIiOjSpSpAS5YsAQD07ds37P5ly5Zh7NixAICFCxfC4XBg+PDh8Hq9GDhwIBYvXqwemM8XRDBod+qk/RwmsexPrQUAK8r+QWKUckqjZmqoqCdLK6bLWhEMD4Nu6q56lq+ivTZnLiD217k6w07XXCWimYTKvjXttVOCVacaKKdhQ5S/TiheQ/pp8vbfs0SxzwK6Uw0i0e95nQcUCUVFRUhKSsIV7a62fR6Qy2W/jmrPp6k3BUj9AtK8OHW7gFM5lqBiLJq2gO48IG1gqD+gOfdG+cFGeb6T5k0rUm8qQP0qQJpPH+rzgCzl/CzL/vaJitKdR+dw2B+LwxHBD6oKgUAAez7cFdnzgIiIiGqLBYiIiIxgASIiIiNYgIiIyAgWICIiMoIFiIiIjGABIiIiI1iAiIjICBYgIiIyotaXY4g0QcD2Obr+gOaMaN04HLB/1rLDbf/KrADgUEXxKK8UqjgjWn3utPbKr4qz+NURNYp0A20EiipCSJvFozwDXRdnpEyT0L4oFDRjieSVXPWLqEuT0Fwk2O/X9R0VpRm8No3FfltNSknA5rbkERARERnBAkREREawABERkREsQEREZAQLEBERGcECRERERrAAERGRESxARERkBAsQEREZwQJERERGsAAREZER9TYLzumKgtNpL2DJ5bS/GJr8NUCb2KWjycnSZLsBunyvoDLHTBk1pmqvzTHT5FOJNmtM016Zj6cZNxDZLDht+0iJ6Di0fWuz44L2P8sHle8qAcv+vqVdTIfifUUU47a7LXkERERERrAAERGRESxARERkBAsQEREZwQJERERGsAAREZERLEBERGQECxARERnBAkREREawABERkRH1Noon2u2C02bEjtNhL7KnNoKqiBVlzI8iN8OhzQapR9EtmtbaiJqgIi5H01Y7lkjH2UQyiieStOs8Uizl68dSxN/873+w3VSUL+VAIKD7DwqWpTgG0cYT2cAjICIiMoIFiIiIjGABIiIiI1iAiIjICBYgIiIyggWIiIiMYAEiIiIjWICIiMgIFiAiIjKCBYiIiIxgASIiIiPqbRZclOWMSMabNmtM11yXH2Up8qO049bENkU6C06zEtV5bYqcrEhmwSl3lIiu8/qUBVdfxu1QvNYAQJSfzUWzbzl0Y9Hsh5r3FABwqKLgFH3bHDOPgIiIyAhVAcrJyUGPHj2QkJCApk2bYujQodi/f39Ym759+8KyrLDbnXfeWaeDJiKihk9VgLZs2YJJkyZh27ZtePfdd+Hz+TBgwACUlJSEtRs/fjyOHj0aus2fP79OB01ERA2f6jeg9evXh/398ssvo2nTpti1axd69+4duj82Nhbp6el1M0IiIroonddvQIWFhQCAlJSUsPuXL1+O1NRUdOrUCTNmzEBpaWmNfXi9XhQVFYXdiIjo4lfrWXDBYBB33303rrvuOnTq1Cl0/+23344WLVogMzMTe/bswX333Yf9+/fjjTfeqLafnJwczJ49u7bDICKiBsqSWs59nDhxIt5++2188MEHaNasWY3t3nvvPfTr1w8HDhxA69atqzzu9Xrh9XpDfxcVFSErKwvdumfDGVX3s8TVl3xWtVdOlVZMmbSUUzc1rQPK6cmRnIbtV15+mNOwq4rkJZy1NOu8Pk3DdmouVQ3AoZnPrHwta9aK06k7dSUqyv64Ne9XgUAAn+35GIWFhUhMTKz5+W33eIbJkyfjrbfewvvvv3/W4gMAvXr1AoAaC5DH44HH46nNMIiIqAFTFSARwV133YU1a9Zg8+bNaNmy5Tn/z+7duwEAGRkZtRogERFdnFQFaNKkSVixYgXWrVuHhIQE5OfnAwCSkpIQExODgwcPYsWKFbjlllvQuHFj7NmzB1OnTkXv3r3RuXPniCwAERE1TKoCtGTJEgCnTzY907JlyzB27Fi43W5s2LABTz/9NEpKSpCVlYXhw4fjwQcfrLMBExHRxUH9FdzZZGVlYcuWLec1oEqBIACbv6ValiJvSjlRQEP9I6rit0hVDhN0P6BHcE7B6bEofohWTxQIanLm6lP+mnIyTD35MT+yGXa6ba8ah/KMk4AoJ8MotqelHIumtXIXV6VXWk77I7H7WmMWHBERGcECRERERrAAERGRESxARERkBAsQEREZwQJERERGsAAREZERLEBERGQECxARERnBAkREREbU/QV36ojPH0BQ7MXPOBz2AyVU1+1Q0lwvQ9teG4GiiZ2JZLwKAAQC9rePpi0ASASXUxdnFLkYmdP9ayKHIjsWjXozbt1LUx19BcX2dyj3Q9X7iiKWDAACfvttHYq4IbvX6eIREBERGcECRERERrAAERGRESxARERkBAsQEREZwQJERERGsAAREZERLEBERGQECxARERnBAkREREawABERkRH1NgtOQ5M3pc0D0+QwabPgNPR5bfazqZRdqzO7Irl9Gmrf2j1F23996TuSVDlzihwzoDafzBVjEV3vlmI5LUs7csVr2V682+lebWY68giIiIiMYAEiIiIjWICIiMgIFiAiIjKCBYiIiIxgASIiIiNYgIiIyAgWICIiMoIFiIiIjGABIiIiI+ptFI/LFQWn097wROwHm2giaoD6E68TyQghbd/BoLa9Yp1rY2E0cSzKcYsiXkUi2DegjJ1RRiVFkmrf0iYCqV4/yvgo5eteFOFKDu3rTbFitEcUgYBiHWrihgL2cnt4BEREREawABERkREsQEREZAQLEBERGcECRERERrAAERGRESxARERkBAsQEREZwQJERERGsAAREZERLEBERGREvc2Cc1gCh8Ne9lAko8YUEU/qvjXxYdpMOleU03bbcp9P1bcme+90e01bZU6WIuNLmwemCyeLXLYbENk8PVUunTYaUTUUXeeWJgNSuX00+Wunx6L4LK/OpbPfNmhpMyPttw347eW7AUAwyCw4IiKqx1QFaMmSJejcuTMSExORmJiI7OxsvP3226HHy8vLMWnSJDRu3Bjx8fEYPnw4jh07VueDJiKihk9VgJo1a4Z58+Zh165d2LlzJ2666SYMGTIEn376KQBg6tSpePPNN7F69Wps2bIFR44cwbBhwyIycCIiatgs0X4Z/T0pKSlYsGABRowYgSZNmmDFihUYMWIEAGDfvn1o3749cnNzcc0119jqr6ioCElJSejaPRvOKHs/UWm+H9deK8XhtF+j9b8B2f8P2ssSRfI3IH+Fbh0GAvb7F5vfHVeK5LbX/IChfRlF8jcgUS5nRH8DUg1F2blmnWgvNqQciqX4LO9Qvpg1zS1L96tKpK53FgwG8O8vD6KwsBCJiYk1tqv1b0CBQACvvfYaSkpKkJ2djV27dsHn86F///6hNu3atUPz5s2Rm5tbYz9erxdFRUVhNyIiuvipC9Ann3yC+Ph4eDwe3HnnnVizZg06dOiA/Px8uN1uJCcnh7VPS0tDfn5+jf3l5OQgKSkpdMvKylIvBBERNTzqAtS2bVvs3r0b27dvx8SJEzFmzBh89tlntR7AjBkzUFhYGLodPny41n0REVHDoT4PyO12o02bNgCAbt26YceOHXjmmWcwcuRIVFRUoKCgIOwo6NixY0hPT6+xP4/HA4/Hox85ERE1aOd9HlAwGITX60W3bt3gcrmwcePG0GP79+/HoUOHkJ2dfb5PQ0REFxnVEdCMGTMwePBgNG/eHMXFxVixYgU2b96Md955B0lJSRg3bhymTZuGlJQUJCYm4q677kJ2drbtGXBERHTpUBWg48ePY/To0Th69CiSkpLQuXNnvPPOO7j55psBAAsXLoTD4cDw4cPh9XoxcOBALF68OCIDD6OapqicdqiYLqud5auZGqqdtqtIzVBPTw5qMoSgm26uyicClHPfIzdVOtLTsDXtVev7dO+2W2pfPpp9y2HpfhFQrUPtuJVxOU7FE2i3j8OhGby2b/tfguneO+2N47zPA6prtToPSJMHpnyDU2UlRbAAaXcsp+L8pQq/X9W3v0I3lkBQcZ5RQHcekCbfTfumUp8KUH0530l5mgmC/ggWIM05Y9oCpDuBCU7YP+9OS1OAtOcBOZ32x60pQMFgAHlffB6584CIiIjOBwsQEREZwQJERERGsAAREZERLEBERGQECxARERnBAkREREawABERkREsQEREZIQ6DTvSKs8QDwTsn52vSkJQnoHeUJMQIPY/WwSU6QOBgDYJQdG/ciyoL0kIEYz5AXTxRxFNQlD2rBqLpdw+qr5VXauTECz1mtH1brul9kquEYoxC/7va/5c+3m9K0DFxcUAgI8/2mF4JEREdD6Ki4uRlJRU4+P1LgsuGAziyJEjSEhICKu4RUVFyMrKwuHDh8+aLdTQcTkvHpfCMgJczotNXSyniKC4uBiZmZlnDTytd0dADocDzZo1q/HxxMTEi3rjV+JyXjwuhWUEuJwXm/NdzrMd+VTiJAQiIjKCBYiIiIxoMAXI4/Fg5syZ8Hg8pocSUVzOi8elsIwAl/NicyGXs95NQiAioktDgzkCIiKiiwsLEBERGcECRERERrAAERGRESxARERkRIMpQIsWLcLll1+O6Oho9OrVC//85z9ND6lOzZo1C5Zlhd3atWtneljn5f3338ett96KzMxMWJaFtWvXhj0uInj44YeRkZGBmJgY9O/fH59//rmZwZ6Hcy3n2LFjq2zbQYMGmRlsLeXk5KBHjx5ISEhA06ZNMXToUOzfvz+sTXl5OSZNmoTGjRsjPj4ew4cPx7FjxwyNuHbsLGffvn2rbM8777zT0IhrZ8mSJejcuXMo7SA7Oxtvv/126PELtS0bRAFatWoVpk2bhpkzZ+LDDz9Ely5dMHDgQBw/ftz00OpUx44dcfTo0dDtgw8+MD2k81JSUoIuXbpg0aJF1T4+f/58PPvss3jhhRewfft2xMXFYeDAgSgvL7/AIz0/51pOABg0aFDYtl25cuUFHOH527JlCyZNmoRt27bh3Xffhc/nw4ABA1BSUhJqM3XqVLz55ptYvXo1tmzZgiNHjmDYsGEGR61nZzkBYPz48WHbc/78+YZGXDvNmjXDvHnzsGvXLuzcuRM33XQThgwZgk8//RTABdyW0gD07NlTJk2aFPo7EAhIZmam5OTkGBxV3Zo5c6Z06dLF9DAiBoCsWbMm9HcwGJT09HRZsGBB6L6CggLxeDyycuVKAyOsG99fThGRMWPGyJAhQ4yMJ1KOHz8uAGTLli0icnrbuVwuWb16dajNv/71LwEgubm5poZ53r6/nCIiffr0kSlTppgbVIQ0atRIXnzxxQu6Lev9EVBFRQV27dqF/v37h+5zOBzo378/cnNzDY6s7n3++efIzMxEq1at8LOf/QyHDh0yPaSIycvLQ35+fth2TUpKQq9evS667QoAmzdvRtOmTdG2bVtMnDgRJ0+eND2k81JYWAgASElJAQDs2rULPp8vbHu2a9cOzZs3b9Db8/vLWWn58uVITU1Fp06dMGPGDJSWlpoYXp0IBAJ47bXXUFJSguzs7Au6LetdGvb3ffPNNwgEAkhLSwu7Py0tDfv27TM0qrrXq1cvvPzyy2jbti2OHj2K2bNn44YbbsDevXuRkJBgenh1Lj8/HwCq3a6Vj10sBg0ahGHDhqFly5Y4ePAgHnjgAQwePBi5ublwOp2mh6cWDAZx991347rrrkOnTp0AnN6ebrcbycnJYW0b8vasbjkB4Pbbb0eLFi2QmZmJPXv24L777sP+/fvxxhtvGByt3ieffILs7GyUl5cjPj4ea9asQYcOHbB79+4Lti3rfQG6VAwePDj0786dO6NXr15o0aIF/vu//xvjxo0zODI6X6NGjQr9+6qrrkLnzp3RunVrbN68Gf369TM4stqZNGkS9u7d2+B/ozyXmpZzwoQJoX9fddVVyMjIQL9+/XDw4EG0bt36Qg+z1tq2bYvdu3ejsLAQr7/+OsaMGYMtW7Zc0DHU+6/gUlNT4XQ6q8zAOHbsGNLT0w2NKvKSk5Nx5ZVX4sCBA6aHEhGV2+5S264A0KpVK6SmpjbIbTt58mS89dZb2LRpU9h1u9LT01FRUYGCgoKw9g11e9a0nNXp1asXADS47el2u9GmTRt069YNOTk56NKlC5555pkLui3rfQFyu93o1q0bNm7cGLovGAxi48aNyM7ONjiyyDp16hQOHjyIjIwM00OJiJYtWyI9PT1suxYVFWH79u0X9XYFgK+++gonT55sUNtWRDB58mSsWbMG7733Hlq2bBn2eLdu3eByucK25/79+3Ho0KEGtT3PtZzV2b17NwA0qO1ZnWAwCK/Xe2G3ZZ1OaYiQ1157TTwej7z88svy2WefyYQJEyQ5OVny8/NND63O3HPPPbJ582bJy8uTf/zjH9K/f39JTU2V48ePmx5arRUXF8tHH30kH330kQCQp556Sj766CP597//LSIi8+bNk+TkZFm3bp3s2bNHhgwZIi1btpSysjLDI9c523IWFxfL7373O8nNzZW8vDzZsGGDdO3aVa644gopLy83PXTbJk6cKElJSbJ582Y5evRo6FZaWhpqc+edd0rz5s3lvffek507d0p2drZkZ2cbHLXeuZbzwIEDMmfOHNm5c6fk5eXJunXrpFWrVtK7d2/DI9e5//77ZcuWLZKXlyd79uyR+++/XyzLkr/97W8icuG2ZYMoQCIizz33nDRv3lzcbrf07NlTtm3bZnpIdWrkyJGSkZEhbrdbLrvsMhk5cqQcOHDA9LDOy6ZNmwRAlduYMWNE5PRU7IceekjS0tLE4/FIv379ZP/+/WYHXQtnW87S0lIZMGCANGnSRFwul7Ro0ULGjx/f4D48Vbd8AGTZsmWhNmVlZfKb3/xGGjVqJLGxsfLjH/9Yjh49am7QtXCu5Tx06JD07t1bUlJSxOPxSJs2beTee++VwsJCswNXuuOOO6RFixbidrulSZMm0q9fv1DxEblw25LXAyIiIiPq/W9ARER0cWIBIiIiI1iAiIjICBYgIiIyggWIiIiMYAEiIiIjWICIiMgIFiAiIjKCBYiIiIxgASIiIiNYgIiIyIj/D4plptYa/hb+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"DATASET\"\"\"\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_DIR = save_dir\n",
    "    LABELS_PATH = DATA_DIR/\"data.txt\"\n",
    "except:\n",
    "    DATA_DIR = save_dir\n",
    "    if not DATA_DIR.exists():\n",
    "        print(\"Dataset doesnt exist or not preprocessed\")\n",
    "    else:\n",
    "        LABELS_PATH = DATA_DIR/\"data.txt\"\n",
    "!pip install imageio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path\n",
    "import imageio as iio\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class DrivingDataset(Dataset):\n",
    "    def __init__(self, labels_path, data_dir, transform=None):\n",
    "        with open(Path(labels_path), \"r\") as f:\n",
    "            self.labels = f.readlines()\n",
    "            f.close()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        img_path, label = self.labels[index].split()\n",
    "        label=float(label)\n",
    "        img = iio.imread(self.data_dir/img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, label)\n",
    "        \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32,32)),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = DrivingDataset(labels_path=LABELS_PATH, data_dir=DATA_DIR, transform=transform)\n",
    "# print(len(dataset))\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = int(len(dataset)-train_size)\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "for i, (imgs, labels) in enumerate(train_loader):\n",
    "    img = imgs[0]\n",
    "    label = labels[0]\n",
    "    plt.imshow(np.transpose(img,  (1, 2, 0)))\n",
    "    plt.title(label)\n",
    "    # plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MODEL\"\"\"\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\"MODEL\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "\n",
    "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.img_size = config[\"img_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        \n",
    "        self.num_patches = (self.img_size//self.patch_size)**2\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(2).transpose(1,2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    # Patch Embeddings + (CLS Token + Positional Embeddings )\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.embed_dim))\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches+1, self.embed_dim))\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1) # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, attention_head_size,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.attention_head_size = attention_head_size\n",
    "        self.bias = config[\"bias\"]\n",
    "        \n",
    "        self.query = nn.Linear(self.embed_dim, self.attention_head_size, bias=self.bias)\n",
    "        self.key = nn.Linear(self.embed_dim, self.attention_head_size, bias=self.bias)\n",
    "        self.value = nn.Linear(self.embed_dim, self.attention_head_size, bias=self.bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1,-2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_scores = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "        attention_out = torch.matmul(attention_scores, v)\n",
    "        \n",
    "        return attention_out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.head_size = self.embed_dim//self.num_heads\n",
    "        self.all_head_size = self.head_size * self.num_heads\n",
    "        self.dropout = config[\"dropout\"]\n",
    "        self.qkv_bias = config[\"bias\"]\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(\n",
    "                self.head_size,\n",
    "                config\n",
    "            ) for _ in range(self.num_heads)\n",
    "        ])\n",
    "\n",
    "        self.attention_mlp = nn.Linear(self.all_head_size, self.embed_dim)\n",
    "        self.out_dropout = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        attention_output = torch.cat([attention_output for attention_output in attention_outputs], dim=-1) #concat attention for each head\n",
    "        attention_output = self.attention_mlp(attention_output)\n",
    "        attention_output = self.out_dropout(attention_output)\n",
    "\n",
    "        return attention_output\n",
    "        \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.fc1 = nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        # self.act = nn.GELU()\n",
    "        self.act=NewGELUActivation()\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        self.dropout=nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim=config[\"embed_dim\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.mlp = MLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        attention_output = self.attention(self.layer_norm1(x))\n",
    "        x = x+attention_output\n",
    "        mlp_out = self.mlp(self.layer_norm2(x))\n",
    "        x = x+mlp_out\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config[\"num_hidden_layers\"])])\n",
    "    def forward(self,x ):\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.img_size = config[\"img_size\"]\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "    \n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder = Encoder(config)\n",
    "    \n",
    "        self.classifier = nn.Linear(self.embed_dim, self.num_classes)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_output = self.embeddings(x)\n",
    "        encoder_output = self.encoder(embedding_output)\n",
    "        classification = self.classifier(encoder_output[:, 0, :])\n",
    "        return classification\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.positional_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.positional_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=0.02,\n",
    "            ).to(module.positional_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=0.02,\n",
    "            ).to(module.cls_token.dtype)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UTILS\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "CHECKPOINT_DIR = root_dir/\"e2e_steering_ckpt\"\n",
    "EXPERIMENT_DIR = root_dir/\"experiments\"\n",
    "\n",
    "def save_checkpoint(state_dict, epoch, path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Creating folder\")\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_details = {\n",
    "        \"epoch\":epoch,\n",
    "        \"state_dict\": state_dict,\n",
    "    }\n",
    "    torch.save(model_details, f\"{p}/vit_cifar10_{epoch}.pth\")\n",
    "    print(f\"model saved at path : {p}/vit_cifar10_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, epoch):\n",
    "    model.load_state_dict(torch.load(f\"{path}/vit_cifar10_{epoch}.pth\")[\"state_dict\"])\n",
    "    return model\n",
    "\n",
    "def save_experiment(model, epoch, config, train_losses, test_losses, accuracies, path):\n",
    "    exp_data = {\n",
    "        \"train_losses\":train_losses,\n",
    "        \"test_losses\":test_losses,\n",
    "        \"accuracies\":accuracies,\n",
    "        \"epoch\":epoch,\n",
    "    }\n",
    "    exp_name = config[\"exp_name\"]\n",
    "    config_file = path/f\"{exp_name}\"/\"config.json\"\n",
    "    metrics_file = path/f\"{exp_name}\"/\"metrics.json\"\n",
    "    files = [config_file , metrics_file]\n",
    "    for file in files:\n",
    "        if file.exists():\n",
    "            print(f\"{file} exists\")\n",
    "        else:\n",
    "            file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            file.touch()\n",
    "            print(f\"{file} created\")\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(exp_data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    save_checkpoint(model.state_dict(), epoch, path/f\"{exp_name}\")\n",
    "\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    with open(path/f\"{exp_name}\"/\"metrics.json\", 'r') as file:\n",
    "      data = json.load(file)\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    accuracies=data[\"accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path/exp_name, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, accuracies, epoch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CONFIG\"\"\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"img_size\":32,\n",
    "    \"patch_size\":4,\n",
    "    \"num_channels\":3,\n",
    "    \"embed_dim\":48*16,\n",
    "    \"dropout\":0.0,\n",
    "    \"bias\":True,\n",
    "    \"num_heads\":4,\n",
    "    \"hidden_dim\":4*48*16,\n",
    "    \"num_hidden_layers\":9,\n",
    "    \"num_classes\":1,\n",
    "    \"device\":\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"exp_name\" : 'e2e--10-epochs',\n",
    "    \"num_epochs\":10,\n",
    "    \"lr\":0.01,\n",
    "    \"save_model_every\" :0\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device, path=CHECKPOINT_DIR):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.path=path\n",
    "        self.exp_dir = EXPERIMENT_DIR\n",
    "\n",
    "    def train(self, train_loader, test_loader, num_epochs, save_model_every_n_epochs=0):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        accuracies = [] \n",
    "\n",
    "        for i in range(num_epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            accuracy, test_loss = self.evaluate(test_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != num_epochs:\n",
    "                print('\\tSave checkpoint at epoch', i+1)\n",
    "                save_checkpoint(self.model.state_dict(), i+1, train_losses, test_losses, accuracies, self.path)\n",
    "\n",
    "        save_experiment(self.model, num_epochs, config, train_losses, test_losses, accuracies, self.exp_dir)\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(train_loader):\n",
    "            imgs = imgs.to(self.device)\n",
    "            labels = torch.tensor(labels).float().to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(imgs)\n",
    "            predictions = predictions.view(predictions.shape[0]).float()\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()* len(imgs)\n",
    "\n",
    "        return total_loss / len(train_loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (imgs, labels) in enumerate(test_loader):\n",
    "                imgs = imgs.to(self.device)\n",
    "                labels = torch.tensor(labels).float().to(self.device)\n",
    "\n",
    "                predictions = self.model(imgs)\n",
    "                predictions = predictions.view(predictions.shape[0]).float()\n",
    "                \n",
    "                loss = self.criterion(predictions, labels)\n",
    "                total_loss += loss.item() * len(imgs)\n",
    "\n",
    "                 # Calculate the accuracy\n",
    "                predictions = torch.argmax(predictions, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        avg_loss = total_loss / len(test_loader.dataset)\n",
    "        return accuracy, avg_loss\n",
    "            \n",
    "\n",
    "def main():\n",
    "    save_model_every_n_epochs = config[\"save_model_every\"]\n",
    "    model = ViT(config)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=1e-2)\n",
    "    criterion = nn.MSELoss()\n",
    "    trainer = Trainer(model, optimizer, criterion, device=config[\"device\"])\n",
    "    trainer.train(train_loader, test_loader, config[\"num_epochs\"], save_model_every_n_epochs=save_model_every_n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32207/3186932329.py:36: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = iio.imread(self.data_dir/img_path)\n",
      "/tmp/ipykernel_32207/98580535.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).float().to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7781.1128, grad_fn=<MseLossBackward0>)\n",
      "tensor(4322.9922, grad_fn=<MseLossBackward0>)\n",
      "tensor(1940.8619, grad_fn=<MseLossBackward0>)\n",
      "tensor(6585692., grad_fn=<MseLossBackward0>)\n",
      "tensor(1896.4395, grad_fn=<MseLossBackward0>)\n",
      "tensor(680854.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(39915.9375, grad_fn=<MseLossBackward0>)\n",
      "tensor(8052607.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(642006.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(375310.0625, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TRAINING\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VIS Loss, ACC\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    data = json.load(path/f\"exp_name\"/\"metrics.json\")\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    accuracies=data[\"accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, accuracies, epoch\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = ViT(config)\n",
    "_, train_losses, test_losses, accuracies,_ = load_experiment(model, config[\"exp_name\"], EXPERIMENT_DIR)\n",
    "# Create two subplots of train/test losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(train_losses, label=\"Train loss\")\n",
    "ax1.plot(test_losses, label=\"Test loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax2.plot(accuracies)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"metrics.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
